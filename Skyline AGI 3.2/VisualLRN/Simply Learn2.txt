import numpy as np

class Environment:
    def __init__(self):
        self.state_space_size = 10
        self.action_space_size = 4
        self.state = 0

    def reset(self):
        self.state = 0
        return self.state

    def step(self, action):
        if action == 0:
            self.state += 1
        elif action == 1:
            self.state -= 1
        elif action == 2:
            self.state += 2
        elif action == 3:
            self.state -= 2

        reward = -abs(self.state - 5)
        return self.state, reward

class QLearningAgent:
    def __init__(self, state_space_size, action_space_size, learning_rate=0.1, discount_factor=0.9,
                 exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay=0.995):
        self.q_table = np.zeros((state_space_size, action_space_size))
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.min_exploration_rate = min_exploration_rate
        self.exploration_decay = exploration_decay
        self.prev_state = None
        self.prev_action = None

    def select_action(self, state):
        if np.random.rand() < self.exploration_rate:
            return np.random.randint(0, self.q_table.shape[1])
        else:
            return np.argmax(self.q_table[state, :])

    def update_q_table(self, state, action, reward, next_state):
        if self.prev_state is not None:
            best_next_action = np.argmax(self.q_table[next_state, :])
            td_error = reward + self.discount_factor * self.q_table[next_state, best_next_action] - self.q_table[self.prev_state, self.prev_action]
            self.q_table[self.prev_state, self.prev_action] += self.learning_rate * td_error

    def take_action(self, state):
        action = self.select_action(state)
        self.prev_state = state
        self.prev_action = action
        return action

    def decay_exploration_rate(self):
        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)

# Example usage
env = Environment()
agent = QLearningAgent(env.state_space_size, env.action_space_size)

num_episodes = 1000
for episode in range(num_episodes):
    state = env.reset()
    total_reward = 0

    for _ in range(50):  # Maximum of 50 steps per episode
        action = agent.take_action(state)
        next_state, reward = env.step(action)
        agent.update_q_table(state, action, reward, next_state)
        agent.decay_exploration_rate()

        state = next_state
        total_reward += reward

    print(f"Episode {episode + 1}, Total Reward: {total_reward}")